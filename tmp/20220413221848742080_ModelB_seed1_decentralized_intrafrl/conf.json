{"modelA": "ModelA", "modelB": "ModelB", "model": "ModelB", "dcntrl": "decentralized", "cntrl": "centralized", "interfrl": "interfrl", "intrafrl": "intrafrl", "nofrl": "normal", "fed_method": "intrafrl", "framework": "decentralized", "fed_enabled": true, "weighted_average_enabled": true, "weighted_window": 1, "fed_update_count": 1, "fed_cutoff_ratio": 1.0, "fed_update_delay": 10.0, "res_dir": ".outputs", "report_dir": "reports", "param_path": "conf.json", "aggregation_method": "weights", "intra_directional_averaging": true, "num_platoons": 2, "pl_size": 2, "pl_leader_reset_a": 0, "reset_max_u": 0.1, "pl_leader_tau": 0.1, "exact": "exact", "euler": "euler", "method": "euler", "timegap": 1.0, "dyn_coeff": 0.1, "reward_ep_coeff": 0.4, "reward_ev_coeff": 0.2, "reward_u_coeff": 0.2, "reward_jerk_coeff": 0.2, "max_ep": 20, "max_ev": 20, "reset_ep_max": 1.5, "reset_max_ev": 1.5, "reset_max_a": 0.05, "reset_ep_eval_max": 1, "reset_ev_eval_max": 1, "reset_a_eval_max": 0.03, "action_high": 2.5, "action_low": -2.5, "re_scalar": 1, "terminal_reward": 0.5, "can_terminate": true, "random_seed": 1, "evaluation_seed": 6, "normal": "normal", "uniform": "uniform", "rand_gen": "normal", "rand_states": true, "total_time_steps": 1600, "sample_rate": 0.1, "episode_sim_time": 60, "steps_per_episode": 600, "fed_update_delay_steps": 100, "number_of_episodes": 2, "gamma": 0.99, "fed_cutoff_episode": 2, "centrl_hidd_mult": 1.2, "reward_averaging_window": 40, "critic_lr": 0.0005, "actor_lr": 5e-05, "std_dev": 0.02, "theta": 0.15, "ou_dt": 0.01, "tau": 0.001, "batch_size": 64, "buffer_size": 100000, "show_env": false, "actor_layer1_size": 256, "actor_layer2_size": 128, "critic_layer1_size": 256, "critic_act_layer_size": 48, "critic_layer2_size": 128, "img_tag": "%s_%s", "actor_fname": "actor%s_%s.h5", "actor_picname": "actor%s_%s.svg", "actor_weights": "actor_weights%s_%s.h5", "critic_fname": "critic%s_%s.h5", "critic_picname": "critic%s_%s.svg", "critic_weights": "critic_weights%s_%s.h5", "t_actor_fname": "target_actor%s_%s.h5", "t_actor_picname": "target_actor%s_%s.svg", "t_actor_weights": "target_actor_weights%s_%s.h5", "t_critic_fname": "target_critic%s_%s.h5", "t_critic_picname": "target_critic%s_%s.svg", "t_critic_weights": "target_critic_weights%s_%s.h5", "pl_tag": "_p%s", "seed_tag": "_seed%s", "fig_path": "reward_curve_p%s.svg", "avg_ep_reward_path": "avg_ep_reward__seed%s.csv", "ep_reward_path": "ep_reward__seed%s.csv", "frl_weighted_avg_parameters_path": "frl_weightings__seed%s.csv", "zerofig_name": "zero", "guasfig_name": "guassian", "stepfig_name": "step", "rampfig_name": "ramp", "dirs": [".outputs", "reports"], "log_format": "%(asctime)s %(name)-12s %(levelname)-8s %(message)s", "log_date_fmt": "%y-%m-%d %H:%M:%S", "pl_rew_for_simulation": -205.3820037841797, "pl_rews_for_simulations": [-220.6230010986328, -190.14100646972656], "index_col": "timestamp", "timestamp": "20220413221848742080", "drop_keys_in_report": ["fed_enabled", "weighted_average_enabled", "modelA", "modelB", "dcntrl", "cntrl", "interfrl", "intrafrl", "hfrl", "vfrl", "nofrl", "res_dir", "report_dir", "param_path", "euler", "exact", "normal", "uniform", "show_env", "actor_fname", "actor_picname", "actor_weights", "critic_fname", "critic_picname", "critic_weights", "t_actor_fname", "t_actor_picname", "t_actor_weights", "t_critic_fname", "t_critic_picname", "t_critic_weights", "fig_path", "zerofig_name", "guasfig_name", "stepfig_name", "rampfig_name", "dirs", "log_format", "log_date_fmt", "drop_keys_in_report", "index_col", "param_descs", "img_tag", "pl_rews_for_simulations", "pl_tag"], "param_descs": {"timestamp": "The time at which the experiment was run", "model": "Whether a 3 (ModelA) of 4 (ModelB) state model", "fed_method": "Type of federated learning used", "framework": "Decentralized or centralized", "weighted_window": "number of episodes to consider for calculating the weight for averaging", "fed_update_count": "The number of episodes between a federated averaging update", "fed_update_delay": "the time in second between updates during a training episode for FRL", "fed_cutoff_ratio": "The percent of all episodes before ending federated updates", "num_platoons": "The number of platoons", "pl_size": "The size of the platoon", "pl_leader_reset_a": "The mean value of the platoon leader acceleration upon environment reset", "reset_max_u": "The mean value of the platoon leader control input upon environment reset", "pl_leader_tau": "Vehicle dynamics coefficient", "method": "Exact or euler discretization", "timegap": "Constant time headway CACC time gap", "dyn_coeff": "Dynamic coefficients for the following vehicles", "reward_ev_coeff": "Coefficient of velocity difference in reward equation", "reward_u_coeff": "Coefficient of control input in reward equation", "max_ep": "Maximum position error in followers before episode termination", "max_ev": "Maximum position error in followers before episode termination", "reset_ep_max": "Maximum position error in followers upon environment reset", "reset_max_ev": "Maximum velocity error in followers upon environment reset", "reset_max_a": "Maximum acceleration upon environment reset", "reset_ep_eval_max": "Position error upon initialization of the evaluator", "reset_ev_eval_max": "Velocity error upon initialization of the evaluator", "reset_a_eval_max": "Accel error upon initialization of the evaluator", "action_high": "Upper bound on action space for the environment", "action_low": "Lower bound on action space for the environment", "re_scalar": "Reward scaling coefficient", "terminal_reward": "Reward assigned upon early termination", "can_terminate": "Whether the environment is allowed to terminate early", "random_seed": "Seed for the experiment across all python libraries", "evaluation_seed": "Seed used during final simulation", "rand_gen": "Either uniform or normal for random number generation", "rand_states": "whether or not to use random initial states for each environment reset", "total_time_steps": "The number of timesteps to train on", "sample_rate": "The sample rate of the system", "episode_sim_time": "The time in seconds to run simulations for", "steps_per_episode": "The number of steps per episode", "fed_update_delay_steps": "The number of steps in an episode between a FRL update", "number_of_episodes": "The total number of episodes for training", "gamma": "The discounted reward coefficient", "fed_cutoff_episode": "The episode at which federated learning terminates", "centrl_hidd_mult": "Multiplier for number of nodes across hidden layers", "critic_lr": "Learning rate for critic network", "actor_lr": "Learning rate for actor network", "std_dev": "Standard deviation used in OU noise", "theta": "Theta value for OU noise", "ou_dt": "Sample rate for OU noise", "tau": "Target network update parameter", "batch_size": "Batch size for sampling replay buffer", "buffer_size": "Size of the replay buffer", "pl_rews_for_simulations": "Stores all platoon's rewards after the final simulation", "pl_rew_for_simulation": "The average of the platoon rewards after the final simulation", "actor_layer1_size": "Number of nodes in actor's first hidden layer", "actor_layer2_size": "Number of nodes in actor's second hidden layer", "critic_layer1_size": "Number of nodes in critic's first hidden layer", "critic_act_layer_size": "Number of nodes in the critic's action layer", "critic_layer2_size": "Number of nodes in critics's second hidden layer"}}